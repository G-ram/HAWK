
\documentclass[12pt]{article}

\setlength{\oddsidemargin}{0pt}
\setlength{\textwidth}{470pt}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{latexsym}
\usepackage{epsfig}
\usepackage[procnames]{listings}
\usepackage{color}
\usepackage{titling}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\algrenewcommand\textproc{}

\setlength{\droptitle}{-2cm}
\setlength{\parskip}{1em} 

\newtheorem*{mydef}{Definition}



\newcommand{\setdocdata}{
\title{HAWK (HTML is All We Know)\\
Language Proposal}
\author{Jonathan Adelson, Ethan Benjamin, Justin Chang,Graham Gobieski,George Yu\\
jma2215,jc4137,eb2947,,gsg2120,gy2206}
\date{}
}


\begin{document}

\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{red}{RGB}{160,0,0}
\definecolor{green}{RGB}{0,150,0}
 
\lstset{language=Python, 
        basicstyle=\ttfamily\small, 
        keywordstyle=\color{keywords},
        commentstyle=\color{comments},
        stringstyle=\color{red},
        showstringspaces=false,
        identifierstyle=\color{green},
        procnamekeys={def,class}}

\setdocdata
\maketitle

\section*{Introduction}

HAWK (HTML is All We Know) is a play on Awk, and strives to accomplish for HTML web scraping what AWK accomplished for text processing. Web scraping describes the process of automatically extracting data from websites. For instance, one could write a web scraping program to look at the menu for John Jay dining hall each day and determine if bacon is being served. As another example, one could scrape IMDB to determine how many degrees of separation a given actor has from Kevin Bacon. 

Though no two web scraping tasks are the same, most web scraping programs employ a similar workflow. For the most part, this involves finding relevant parts of a web page and performing some action in response. In practice, the most typical ``relevant parts" are HTML elements that match some criteria or certain strings in the raw HTML document. Typically, in order to find these parts you must combine various search mechanisms including XPath, CSS selector search, and regexes. Often, these distinct search mechanisms are implemented in separate libraries which each have distinct abstractions that don't play well together. 

Like Awk, HAWK mostly consists pattern-action pairs. HAWK supports multiple types of patterns, and all are unified under one coherent language construct. For our project, we will support three types of search patterns for HTML documents: CSS selectors, regular expressions, and HAWK predicates. CSS selectors and HAWK predicates will allow users to match whole HTML elements. HAWK predicates are simply boolean expressions with the same syntax and semantics as HAWK's action language. Regular expressions will let users cut right to the chase and scrape raw string data. 

HAWK's action syntax and semantics will be bare-bones and dynamic, in the spirit of Lua. We hope to provide just enough features to make the large majority of tasks straightforward, and enough flexibility to make hard tasks possible. Like Lua, we will provide only one built-in data structure, a table, which is essentially just a key-value hash table. 




\section*{References}

 \begin{thebibliography}{1}
  
   \bibitem{F} Vitaly Feldman  {\em Evolvability from Learning Algorithms.}  2008.


  \end{thebibliography}

\end{document}


